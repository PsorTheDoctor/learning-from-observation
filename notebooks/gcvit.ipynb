{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMA/+hx+awW47SmbEybNJQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsorTheDoctor/learning-from-observation/blob/master/notebooks/gcvit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GCViT: Global Context Vision Transformer"
      ],
      "metadata": {
        "id": "gBEWM2g1YQUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras_cv tensorflow\n",
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "id": "JVDl3FlGccAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # \"tensorflow\", \"jax\", \"torch\"\n",
        "import keras\n",
        "from keras_cv.layers import DropPath\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "import tensorflow as tf  # only for dataloader\n",
        "from skimage.data import chelsea\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qTJ70VBlTkns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mCe6BnQ_XmCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958ae733-1719-42a2-b44a-798da0da32af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    x_train = np.load(path + 'train/images.npy')\n",
        "    y_train = np.load(path + 'train/joints.npy')\n",
        "    x_test = np.load(path + 'test/images.npy')\n",
        "    y_test = np.load(path + 'test/joints.npy')\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "Vwp7TVOrXru9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'drive/MyDrive/learning-from-observation/data/snake/'\n",
        "num_classes = 2\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_data(path)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "v7EOodUfXtwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937cd2af-5aeb-470a-f5ee-7ef13aeb94a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (4000, 224, 224, 3) - y_train shape: (4000, 2)\n",
            "x_test shape: (1000, 224, 224, 3) - y_test shape: (1000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeAndExcitation(layers.Layer):\n",
        "    def __init__(self, output_dim=None, expansion=0.25, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.expansion = expansion\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        inp = input_shape[-1]\n",
        "        self.output_dim = self.output_dim or inp\n",
        "        self.avg_pool = layers.GlobalAvgPool2D(keepdims=True, name=\"avg_pool\")\n",
        "        self.fc = [\n",
        "            layers.Dense(int(inp * self.expansion), use_bias=False, name=\"fc_0\"),\n",
        "            layers.Activation(\"gelu\", name=\"fc_1\"),\n",
        "            layers.Dense(self.output_dim, use_bias=False, name=\"fc_2\"),\n",
        "            layers.Activation(\"sigmoid\", name=\"fc_3\"),\n",
        "        ]\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.avg_pool(inputs)\n",
        "        for layer in self.fc:\n",
        "            x = layer(x)\n",
        "        return x * inputs\n",
        "\n",
        "class ReduceSize(layers.Layer):\n",
        "    def __init__(self, keepdims=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.keepdims = keepdims\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        embed_dim = input_shape[-1]\n",
        "        dim_out = embed_dim if self.keepdims else 2 * embed_dim\n",
        "        self.pad1 = layers.ZeroPadding2D(1, name=\"pad1\")\n",
        "        self.pad2 = layers.ZeroPadding2D(1, name=\"pad2\")\n",
        "        self.conv = [\n",
        "            layers.DepthwiseConv2D(\n",
        "                kernel_size=3, strides=1, padding=\"valid\", use_bias=False, name=\"conv_0\"\n",
        "            ),\n",
        "            layers.Activation(\"gelu\", name=\"conv_1\"),\n",
        "            SqueezeAndExcitation(name=\"conv_2\"),\n",
        "            layers.Conv2D(\n",
        "                embed_dim,\n",
        "                kernel_size=1,\n",
        "                strides=1,\n",
        "                padding=\"valid\",\n",
        "                use_bias=False,\n",
        "                name=\"conv_3\",\n",
        "            ),\n",
        "        ]\n",
        "        self.reduction = layers.Conv2D(\n",
        "            dim_out,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding=\"valid\",\n",
        "            use_bias=False,\n",
        "            name=\"reduction\",\n",
        "        )\n",
        "        self.norm1 = layers.LayerNormalization(\n",
        "            -1, 1e-05, name=\"norm1\"\n",
        "        )  # eps like PyTorch\n",
        "        self.norm2 = layers.LayerNormalization(-1, 1e-05, name=\"norm2\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.norm1(inputs)\n",
        "        xr = self.pad1(x)\n",
        "        for layer in self.conv:\n",
        "            xr = layer(xr)\n",
        "        x = x + xr\n",
        "        x = self.pad2(x)\n",
        "        x = self.reduction(x)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class MLP(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_features=None,\n",
        "        out_features=None,\n",
        "        activation=\"gelu\",\n",
        "        dropout=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_features = hidden_features\n",
        "        self.out_features = out_features\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.in_features = input_shape[-1]\n",
        "        self.hidden_features = self.hidden_features or self.in_features\n",
        "        self.out_features = self.out_features or self.in_features\n",
        "        self.fc1 = layers.Dense(self.hidden_features, name=\"fc1\")\n",
        "        self.act = layers.Activation(self.activation, name=\"act\")\n",
        "        self.fc2 = layers.Dense(self.out_features, name=\"fc2\")\n",
        "        self.drop1 = layers.Dropout(self.dropout, name=\"drop1\")\n",
        "        self.drop2 = layers.Dropout(self.dropout, name=\"drop2\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HxuEwOIYTwe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.pad = layers.ZeroPadding2D(1, name=\"pad\")\n",
        "        self.proj = layers.Conv2D(self.embed_dim, 3, 2, name=\"proj\")\n",
        "        self.conv_down = ReduceSize(keepdims=True, name=\"conv_down\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.pad(inputs)\n",
        "        x = self.proj(x)\n",
        "        x = self.conv_down(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oVPfPg7uXuaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtraction(layers.Layer):\n",
        "    def __init__(self, keepdims=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.keepdims = keepdims\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        embed_dim = input_shape[-1]\n",
        "        self.pad1 = layers.ZeroPadding2D(1, name=\"pad1\")\n",
        "        self.pad2 = layers.ZeroPadding2D(1, name=\"pad2\")\n",
        "        self.conv = [\n",
        "            layers.DepthwiseConv2D(3, 1, use_bias=False, name=\"conv_0\"),\n",
        "            layers.Activation(\"gelu\", name=\"conv_1\"),\n",
        "            SqueezeAndExcitation(name=\"conv_2\"),\n",
        "            layers.Conv2D(embed_dim, 1, 1, use_bias=False, name=\"conv_3\"),\n",
        "        ]\n",
        "        if not self.keepdims:\n",
        "            self.pool = layers.MaxPool2D(3, 2, name=\"pool\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        xr = self.pad1(x)\n",
        "        for layer in self.conv:\n",
        "            xr = layer(xr)\n",
        "        x = x + xr\n",
        "        if not self.keepdims:\n",
        "            x = self.pool(self.pad2(x))\n",
        "        return x\n",
        "\n",
        "class GlobalQueryGenerator(layers.Layer):\n",
        "    def __init__(self, keepdims=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.keepdims = keepdims\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.to_q_global = [\n",
        "            FeatureExtraction(keepdims, name=f\"to_q_global_{i}\")\n",
        "            for i, keepdims in enumerate(self.keepdims)\n",
        "        ]\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        for layer in self.to_q_global:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bqc7MkpXauqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        window_size,\n",
        "        num_heads,\n",
        "        global_query,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        attention_dropout=0.0,\n",
        "        projection_dropout=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        window_size = (window_size, window_size)\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.global_query = global_query\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.projection_dropout = projection_dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        embed_dim = input_shape[0][-1]\n",
        "        head_dim = embed_dim // self.num_heads\n",
        "        self.scale = self.qk_scale or head_dim**-0.5\n",
        "        self.qkv_size = 3 - int(self.global_query)\n",
        "        self.qkv = layers.Dense(\n",
        "            embed_dim * self.qkv_size, use_bias=self.qkv_bias, name=\"qkv\"\n",
        "        )\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            name=\"relative_position_bias_table\",\n",
        "            shape=[\n",
        "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
        "                self.num_heads,\n",
        "            ],\n",
        "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "            trainable=True,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.attn_drop = layers.Dropout(self.attention_dropout, name=\"attn_drop\")\n",
        "        self.proj = layers.Dense(embed_dim, name=\"proj\")\n",
        "        self.proj_drop = layers.Dropout(self.projection_dropout, name=\"proj_drop\")\n",
        "        self.softmax = layers.Activation(\"softmax\", name=\"softmax\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def get_relative_position_index(self):\n",
        "        coords_h = ops.arange(self.window_size[0])\n",
        "        coords_w = ops.arange(self.window_size[1])\n",
        "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
        "        coords_flatten = ops.reshape(coords, [2, -1])\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
        "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
        "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
        "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
        "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
        "        return relative_position_index\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.global_query:\n",
        "            inputs, q_global = inputs\n",
        "            B = ops.shape(q_global)[0]  # B, N, C\n",
        "        else:\n",
        "            inputs = inputs[0]\n",
        "        B_, N, C = ops.shape(inputs)  # B*num_window, num_tokens, channels\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = ops.reshape(\n",
        "            qkv, [B_, N, self.qkv_size, self.num_heads, C // self.num_heads]\n",
        "        )\n",
        "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4])\n",
        "        if self.global_query:\n",
        "            k, v = ops.split(\n",
        "                qkv, indices_or_sections=2, axis=0\n",
        "            )  # for unknown shame num=None will throw error\n",
        "            q_global = ops.repeat(\n",
        "                q_global, repeats=B_ // B, axis=0\n",
        "            )  # num_windows = B_//B => q_global same for all windows in a img\n",
        "            q = ops.reshape(\n",
        "                q_global, new_shape=[B_, N, self.num_heads, C // self.num_heads]\n",
        "            )\n",
        "            q = ops.transpose(q, axes=[0, 2, 1, 3])\n",
        "        else:\n",
        "            q, k, v = ops.split(qkv, indices_or_sections=3, axis=0)\n",
        "            q = ops.squeeze(q, axis=0)\n",
        "\n",
        "        k = ops.squeeze(k, axis=0)\n",
        "        v = ops.squeeze(v, axis=0)\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = q @ ops.transpose(k, axes=[0, 1, 3, 2])\n",
        "        relative_position_bias = ops.take(\n",
        "            self.relative_position_bias_table,\n",
        "            ops.reshape(self.get_relative_position_index(), new_shape=[-1]),\n",
        "        )\n",
        "        relative_position_bias = ops.reshape(\n",
        "            relative_position_bias,\n",
        "            new_shape=[\n",
        "                self.window_size[0] * self.window_size[1],\n",
        "                self.window_size[0] * self.window_size[1],\n",
        "                -1,\n",
        "            ],\n",
        "        )\n",
        "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
        "        attn = attn + relative_position_bias[None,]\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = ops.transpose((attn @ v), axes=[0, 2, 1, 3])\n",
        "        x = ops.reshape(x, new_shape=[B_, N, C])\n",
        "        x = self.proj_drop(self.proj(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HBWrYKt_a2Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        window_size,\n",
        "        num_heads,\n",
        "        global_query,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        dropout=0.0,\n",
        "        attention_dropout=0.0,\n",
        "        path_drop=0.0,\n",
        "        activation=\"gelu\",\n",
        "        layer_scale=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.global_query = global_query\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.dropout = dropout\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.path_drop = path_drop\n",
        "        self.activation = activation\n",
        "        self.layer_scale = layer_scale\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        B, H, W, C = input_shape[0]\n",
        "        self.norm1 = layers.LayerNormalization(-1, 1e-05, name=\"norm1\")\n",
        "        self.attn = WindowAttention(\n",
        "            window_size=self.window_size,\n",
        "            num_heads=self.num_heads,\n",
        "            global_query=self.global_query,\n",
        "            qkv_bias=self.qkv_bias,\n",
        "            qk_scale=self.qk_scale,\n",
        "            attention_dropout=self.attention_dropout,\n",
        "            projection_dropout=self.dropout,\n",
        "            name=\"attn\",\n",
        "        )\n",
        "        self.drop_path1 = DropPath(self.path_drop)\n",
        "        self.drop_path2 = DropPath(self.path_drop)\n",
        "        self.norm2 = layers.LayerNormalization(-1, 1e-05, name=\"norm2\")\n",
        "        self.mlp = MLP(\n",
        "            hidden_features=int(C * self.mlp_ratio),\n",
        "            dropout=self.dropout,\n",
        "            activation=self.activation,\n",
        "            name=\"mlp\",\n",
        "        )\n",
        "        if self.layer_scale is not None:\n",
        "            self.gamma1 = self.add_weight(\n",
        "                name=\"gamma1\",\n",
        "                shape=[C],\n",
        "                initializer=keras.initializers.Constant(self.layer_scale),\n",
        "                trainable=True,\n",
        "                dtype=self.dtype,\n",
        "            )\n",
        "            self.gamma2 = self.add_weight(\n",
        "                name=\"gamma2\",\n",
        "                shape=[C],\n",
        "                initializer=keras.initializers.Constant(self.layer_scale),\n",
        "                trainable=True,\n",
        "                dtype=self.dtype,\n",
        "            )\n",
        "        else:\n",
        "            self.gamma1 = 1.0\n",
        "            self.gamma2 = 1.0\n",
        "        self.num_windows = int(H // self.window_size) * int(W // self.window_size)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.global_query:\n",
        "            inputs, q_global = inputs\n",
        "        else:\n",
        "            inputs = inputs[0]\n",
        "        B, H, W, C = ops.shape(inputs)\n",
        "        x = self.norm1(inputs)\n",
        "        # create windows and concat them in batch axis\n",
        "        x = self.window_partition(x, self.window_size)  # (B_, win_h, win_w, C)\n",
        "        # flatten patch\n",
        "        x = ops.reshape(x, new_shape=[-1, self.window_size * self.window_size, C])\n",
        "        # attention\n",
        "        if self.global_query:\n",
        "            x = self.attn([x, q_global])\n",
        "        else:\n",
        "            x = self.attn([x])\n",
        "        # reverse window partition\n",
        "        x = self.window_reverse(x, self.window_size, H, W, C)\n",
        "        # FFN\n",
        "        x = inputs + self.drop_path1(x * self.gamma1)\n",
        "        x = x + self.drop_path2(self.gamma2 * self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "    def window_partition(self, x, window_size):\n",
        "        B, H, W, C = ops.shape(x)\n",
        "        x = ops.reshape(\n",
        "            x,\n",
        "            new_shape=[\n",
        "                -1,\n",
        "                H // window_size,\n",
        "                window_size,\n",
        "                W // window_size,\n",
        "                window_size,\n",
        "                C,\n",
        "            ],\n",
        "        )\n",
        "        x = ops.transpose(x, axes=[0, 1, 3, 2, 4, 5])\n",
        "        windows = ops.reshape(x, new_shape=[-1, window_size, window_size, C])\n",
        "        return windows\n",
        "\n",
        "    def window_reverse(self, windows, window_size, H, W, C):\n",
        "        x = ops.reshape(\n",
        "            windows,\n",
        "            new_shape=[\n",
        "                -1,\n",
        "                H // window_size,\n",
        "                W // window_size,\n",
        "                window_size,\n",
        "                window_size,\n",
        "                C,\n",
        "            ],\n",
        "        )\n",
        "        x = ops.transpose(x, axes=[0, 1, 3, 2, 4, 5])\n",
        "        x = ops.reshape(x, new_shape=[-1, H, W, C])\n",
        "        return x"
      ],
      "metadata": {
        "id": "dw4GfvfFa8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Level(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        depth,\n",
        "        num_heads,\n",
        "        window_size,\n",
        "        keepdims,\n",
        "        downsample=True,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        dropout=0.0,\n",
        "        attention_dropout=0.0,\n",
        "        path_drop=0.0,\n",
        "        layer_scale=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.keepdims = keepdims\n",
        "        self.downsample = downsample\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.dropout = dropout\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.path_drop = path_drop\n",
        "        self.layer_scale = layer_scale\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        path_drop = (\n",
        "            [self.path_drop] * self.depth\n",
        "            if not isinstance(self.path_drop, list)\n",
        "            else self.path_drop\n",
        "        )\n",
        "        self.blocks = [\n",
        "            Block(\n",
        "                window_size=self.window_size,\n",
        "                num_heads=self.num_heads,\n",
        "                global_query=bool(i % 2),\n",
        "                mlp_ratio=self.mlp_ratio,\n",
        "                qkv_bias=self.qkv_bias,\n",
        "                qk_scale=self.qk_scale,\n",
        "                dropout=self.dropout,\n",
        "                attention_dropout=self.attention_dropout,\n",
        "                path_drop=path_drop[i],\n",
        "                layer_scale=self.layer_scale,\n",
        "                name=f\"blocks_{i}\",\n",
        "            )\n",
        "            for i in range(self.depth)\n",
        "        ]\n",
        "        self.down = ReduceSize(keepdims=False, name=\"downsample\")\n",
        "        self.q_global_gen = GlobalQueryGenerator(self.keepdims, name=\"q_global_gen\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = inputs\n",
        "        q_global = self.q_global_gen(x)  # shape: (B, win_size, win_size, C)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            if i % 2:\n",
        "                x = blk([x, q_global])  # shape: (B, H, W, C)\n",
        "            else:\n",
        "                x = blk([x])  # shape: (B, H, W, C)\n",
        "        if self.downsample:\n",
        "            x = self.down(x)  # shape: (B, H//2, W//2, 2*C)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fQQDy6LtbEHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCViT(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        window_size,\n",
        "        embed_dim,\n",
        "        depths,\n",
        "        num_heads,\n",
        "        drop_rate=0.0,\n",
        "        mlp_ratio=3.0,\n",
        "        qkv_bias=True,\n",
        "        qk_scale=None,\n",
        "        attention_dropout=0.0,\n",
        "        path_drop=0.1,\n",
        "        layer_scale=None,\n",
        "        num_classes=num_classes,\n",
        "        head_activation=\"softmax\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.window_size = window_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depths = depths\n",
        "        self.num_heads = num_heads\n",
        "        self.drop_rate = drop_rate\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.path_drop = path_drop\n",
        "        self.layer_scale = layer_scale\n",
        "        self.num_classes = num_classes\n",
        "        self.head_activation = head_activation\n",
        "\n",
        "        self.patch_embed = PatchEmbed(embed_dim=embed_dim, name=\"patch_embed\")\n",
        "        self.pos_drop = layers.Dropout(drop_rate, name=\"pos_drop\")\n",
        "        path_drops = np.linspace(0.0, path_drop, sum(depths))\n",
        "        keepdims = [(0, 0, 0), (0, 0), (1,), (1,)]\n",
        "        self.levels = []\n",
        "        for i in range(len(depths)):\n",
        "            path_drop = path_drops[sum(depths[:i]) : sum(depths[: i + 1])].tolist()\n",
        "            level = Level(\n",
        "                depth=depths[i],\n",
        "                num_heads=num_heads[i],\n",
        "                window_size=window_size[i],\n",
        "                keepdims=keepdims[i],\n",
        "                downsample=(i < len(depths) - 1),\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias,\n",
        "                qk_scale=qk_scale,\n",
        "                dropout=drop_rate,\n",
        "                attention_dropout=attention_dropout,\n",
        "                path_drop=path_drop,\n",
        "                layer_scale=layer_scale,\n",
        "                name=f\"levels_{i}\",\n",
        "            )\n",
        "            self.levels.append(level)\n",
        "        self.norm = layers.LayerNormalization(axis=-1, epsilon=1e-05, name=\"norm\")\n",
        "        self.pool = layers.GlobalAvgPool2D(name=\"pool\")\n",
        "        self.head = layers.Dense(num_classes, name=\"head\", activation=head_activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        x = self.patch_embed(inputs)  # shape: (B, H, W, C)\n",
        "        x = self.pos_drop(x)\n",
        "        for level in self.levels:\n",
        "            x = level(x)  # shape: (B, H_, W_, C_)\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x)  # shape: (B, C__)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def build_graph(self, input_shape=(224, 224, 3)):\n",
        "        x = keras.Input(shape=input_shape)\n",
        "        return keras.Model(inputs=[x], outputs=self.call(x), name=self.name)\n",
        "\n",
        "    def summary(self, input_shape=(224, 224, 3)):\n",
        "        return self.build_graph(input_shape).summary()"
      ],
      "metadata": {
        "id": "jdinA3k_bQ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"window_size\": (7, 7, 14, 7),\n",
        "    \"embed_dim\": 64,\n",
        "    \"depths\": (2, 2, 6, 2),\n",
        "    \"num_heads\": (2, 4, 8, 16),\n",
        "    \"mlp_ratio\": 3.0,\n",
        "    \"path_drop\": 0.2,\n",
        "}\n",
        "model = GCViT(**config)\n",
        "inp = ops.array(np.random.uniform(size=(1, 224, 224, 3)))\n",
        "out = model(inp)\n",
        "\n",
        "model.summary((224, 224, 3))"
      ],
      "metadata": {
        "id": "zhlQLE_Bbgkm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "9c6c70f5-efd9-44bb-bda8-e7cda808b899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gc_vi_t\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gc_vi_t\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │           \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ patch_embed (\u001b[38;5;33mPatchEmbed\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │      \u001b[38;5;34m45,632\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ pos_drop (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │           \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_0 (\u001b[38;5;33mLevel\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │     \u001b[38;5;34m180,964\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_1 (\u001b[38;5;33mLevel\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │     \u001b[38;5;34m688,456\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_2 (\u001b[38;5;33mLevel\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │   \u001b[38;5;34m5,170,608\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_3 (\u001b[38;5;33mLevel\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │   \u001b[38;5;34m5,395,744\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ norm (\u001b[38;5;33mLayerNormalization\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │       \u001b[38;5;34m1,024\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ pool (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │           \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ head (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                     │       \u001b[38;5;34m2,052\u001b[0m │\n",
              "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                       </span>┃<span style=\"font-weight: bold\"> Output Shape                  </span>┃<span style=\"font-weight: bold\">     Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ patch_embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbed</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">45,632</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ pos_drop (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Level</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">180,964</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Level</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">688,456</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Level</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │   <span style=\"color: #00af00; text-decoration-color: #00af00\">5,170,608</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ levels_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Level</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │   <span style=\"color: #00af00; text-decoration-color: #00af00\">5,395,744</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────┼───────────────────────────────┼─────────────┤\n",
              "│ head (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
              "└────────────────────────────────────┴───────────────────────────────┴─────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,484,480\u001b[0m (43.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,484,480</span> (43.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,484,480\u001b[0m (43.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,484,480</span> (43.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate= 0.001, weight_decay= 0.0001\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.Huber(),\n",
        "        metrics=[\n",
        "            keras.metrics.MeanSquaredError(),\n",
        "            keras.metrics.MeanAbsoluteError()\n",
        "        ]\n",
        "    )\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=32,\n",
        "        epochs=1,\n",
        "        validation_data=(x_test, y_test)\n",
        "    )\n",
        "    return history"
      ],
      "metadata": {
        "id": "sYZgWRNub3IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Snake"
      ],
      "metadata": {
        "id": "q35ZNQi-ACzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_snake = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9fVdjJakeDR",
        "outputId": "8fc01d82-00a2-41b4-9b0a-6c41b402d6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4802s\u001b[0m 38s/step - loss: 0.3735 - mean_absolute_error: 0.7468 - mean_squared_error: 0.8278 - val_loss: 0.3550 - val_mean_absolute_error: 0.7205 - val_mean_squared_error: 0.7883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = model.predict(x_train)\n",
        "y_pred_test = model.predict(x_test)\n",
        "np.save('gcvit_snake_train.npy', y_pred_train)\n",
        "np.save('gcvit_snake_test.npy', y_pred_test)\n",
        "\n",
        "pred = pd.concat([\n",
        "    pd.DataFrame(np.squeeze(y_pred_test), columns=['Pred joint 1', 'Pred joint 2']),\n",
        "    pd.DataFrame(np.squeeze(y_test), columns=['True joint 1', 'True joint 2']),\n",
        "    pd.DataFrame(np.squeeze(np.abs(y_test - y_pred_test)), columns=['Error joint 1', 'Error joint 2'])\n",
        "], axis=1)\n",
        "pred.head()"
      ],
      "metadata": {
        "id": "xXh1wb5t1f3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train = np.mean(np.abs(y_train - y_pred_train))\n",
        "std_train = np.std(np.abs(y_train - y_pred_train))\n",
        "mean_test = np.mean(np.abs(y_test - y_pred_test))\n",
        "std_test = np.std(np.abs(y_test - y_pred_test))\n",
        "\n",
        "print('Train mean:', mean_train)\n",
        "print('Train std dev:', std_train)\n",
        "print('Test mean:', mean_test)\n",
        "print('Test std dev:', std_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMgJSqoo1m1x",
        "outputId": "0afa5d4a-53bb-435a-f2bc-19a5f8a6d430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train mean: 0.7468549\n",
            "Train std dev: 0.51605886\n",
            "Test mean: 0.7204924\n",
            "Test std dev: 0.5188024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Turtle"
      ],
      "metadata": {
        "id": "dd5-EbRuAHdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'drive/MyDrive/learning-from-observation/data/turtle/'\n",
        "num_classes = 4\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_data(path)\n",
        "\n",
        "# Remove fixed joints\n",
        "y_train = y_train[:, :4]\n",
        "y_test = y_test[:, :4]"
      ],
      "metadata": {
        "id": "QRa_lF3wAlaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_turtle = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruo8zzsZAjD0",
        "outputId": "f0b83c74-4c4e-493e-80be-69d860da5071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4955s\u001b[0m 39s/step - loss: 0.1249 - mean_absolute_error: 0.3041 - mean_squared_error: 0.2507 - val_loss: 0.1292 - val_mean_absolute_error: 0.3071 - val_mean_squared_error: 0.2591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = model.predict(x_train)\n",
        "y_pred_test = model.predict(x_test)\n",
        "np.save('gcvit_turtle_train.npy', y_pred_train)\n",
        "np.save('gcvit_turtle_test.npy', y_pred_test)\n",
        "\n",
        "pred = pd.concat([\n",
        "    pd.DataFrame(np.squeeze(y_pred_test),\n",
        "                 columns=['pred_joint_1', 'pred_joint_2', 'pred_joint_3', 'pred_joint_4']),\n",
        "    pd.DataFrame(np.squeeze(y_test),\n",
        "                 columns=['true_joint_1', 'true_joint_2', 'true_joint_3', 'true_joint_4']),\n",
        "    pd.DataFrame(np.squeeze(np.abs(y_pred_test - y_test)),\n",
        "                 columns=['error_joint_1', 'error_joint_2', 'error_joint_1', 'error_joint_2'])], axis=1)\n",
        "pred.head()"
      ],
      "metadata": {
        "id": "E5lW8Tz9AJmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train = np.mean(np.abs(y_train - y_pred_train))\n",
        "std_train = np.std(np.abs(y_train - y_pred_train))\n",
        "mean_test = np.mean(np.abs(y_test - y_pred_test))\n",
        "std_test = np.std(np.abs(y_test - y_pred_test))\n",
        "\n",
        "print('Train mean:', mean_train)\n",
        "print('Train std dev:', std_train)\n",
        "print('Test mean:', mean_test)\n",
        "print('Test std dev:', std_test)"
      ],
      "metadata": {
        "id": "080-QLJUBwxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}